{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b095b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from run_nerf_helpers import *\n",
    "from load_llff import load_llff_data\n",
    "from load_deepvoxels import load_dv_data\n",
    "from load_blender import load_blender_data\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "debugger = None\n",
    "\n",
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "\n",
    "    def ret(inputs):\n",
    "        return tf.concat([fn(inputs[i:i + chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024 * 64):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\"\"\"\n",
    "\n",
    "    inputs_flat = tf.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "    if viewdirs is not None:\n",
    "        input_dirs = tf.broadcast_to(viewdirs[:, None], inputs.shape)\n",
    "        input_dirs_flat = tf.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = tf.concat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "    outputs = tf.reshape(outputs_flat, list(\n",
    "        inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def raw2outputs(raw, z_vals, rays_d):\n",
    "        \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "\n",
    "        Args:\n",
    "          raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "          z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "          rays_d: [num_rays, 3]. Direction of each ray.\n",
    "\n",
    "        Returns:\n",
    "          rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "          disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "          acc_map: [num_rays]. Sum of weights along each ray.\n",
    "          weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "          depth_map: [num_rays]. Estimated distance to object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Function for computing density from model prediction. This value is\n",
    "        # strictly between [0, 1].\n",
    "        def raw2alpha(raw, dists, act_fn=tf.nn.relu):\n",
    "            return 1.0 - \\\n",
    "                tf.exp(-act_fn(raw) * dists)\n",
    "\n",
    "        # Compute 'distance' (in time) between each integration time along a ray.\n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\n",
    "        # The 'distance' from the last integration time is infinity.\n",
    "        dists = tf.concat(\n",
    "            [dists, tf.broadcast_to([1e10], dists[..., :1].shape)],\n",
    "            axis=-1)  # [N_rays, N_samples]\n",
    "\n",
    "        # Multiply each distance by the norm of its corresponding direction ray\n",
    "        # to convert to real world distance (accounts for non-unit directions).\n",
    "        dists = dists * tf.linalg.norm(rays_d[..., None, :], axis=-1)\n",
    "\n",
    "        # Extract RGB of each sample position along each ray.\n",
    "        rgb = tf.math.sigmoid(raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "\n",
    "        # Add noise to model's predictions for density. Can be used to\n",
    "        # regularize network during training (prevents floater artifacts).\n",
    "        noise = 0.\n",
    "        if raw_noise_std > 0.:\n",
    "            noise = tf.random.normal(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "        # Predict density of each sample along each ray. Higher values imply\n",
    "        # higher likelihood of being absorbed at this point.\n",
    "        alpha = raw2alpha(raw[..., 3] + noise, dists)  # [N_rays, N_samples]\n",
    "\n",
    "        # Compute weight for RGB of each sample along each ray.  A cumprod() is\n",
    "        # used to express the idea of the ray not having reflected up to this\n",
    "        # sample yet.\n",
    "        # [N_rays, N_samples]\n",
    "        weights = alpha * \\\n",
    "                  tf.math.cumprod(1. - alpha + 1e-10, axis=-1, exclusive=True)\n",
    "\n",
    "        # Computed weighted color of each sample along each ray.\n",
    "        rgb_map = tf.reduce_sum(\n",
    "            weights[..., None] * rgb, axis=-2)  # [N_rays, 3]\n",
    "\n",
    "        # Estimated depth map is expected distance.\n",
    "        depth_map = tf.reduce_sum(weights * z_vals, axis=-1)\n",
    "\n",
    "        # Disparity map is inverse depth.\n",
    "        disp_map = 1. / tf.maximum(1e-10, depth_map /\n",
    "                                   tf.reduce_sum(weights, axis=-1))\n",
    "\n",
    "        # Sum of weights along each ray. This value is in [0, 1] up to numerical error.\n",
    "        acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "        # To composite onto a white background, use the accumulated alpha map.\n",
    "        if white_bkgd:\n",
    "            rgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "        return rgb_map, disp_map, acc_map, weights, depth_map\n",
    "\n",
    "    ###############################\n",
    "    # batch size\n",
    "    N_rays = ray_batch.shape[0]\n",
    "\n",
    "    # Extract ray origin, direction.\n",
    "    rays_o, rays_d = ray_batch[:, 0:3], ray_batch[:, 3:6]  # [N_rays, 3] each\n",
    "\n",
    "    # Extract unit-normalized viewing direction.\n",
    "    viewdirs = ray_batch[:, -3:] if ray_batch.shape[-1] > 8 else None\n",
    "\n",
    "    # Extract lower, upper bound for ray distance.\n",
    "    bounds = tf.reshape(ray_batch[..., 6:8], [-1, 1, 2])\n",
    "    near, far = bounds[..., 0], bounds[..., 1]  # [-1,1]\n",
    "\n",
    "    # Decide where to sample along each ray. Under the logic, all rays will be sampled at\n",
    "    # the same times.\n",
    "    t_vals = tf.linspace(0., 1., N_samples)\n",
    "    if not lindisp:\n",
    "        # Space integration times linearly between 'near' and 'far'. Same\n",
    "        # integration points will be used for all rays.\n",
    "        z_vals = near * (1. - t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        # Sample linearly in inverse depth (disparity).\n",
    "        z_vals = 1. / (1. / near * (1. - t_vals) + 1. / far * (t_vals))\n",
    "    z_vals = tf.broadcast_to(z_vals, [N_rays, N_samples])\n",
    "\n",
    "    # Perturb sampling time along each ray.\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        upper = tf.concat([mids, z_vals[..., -1:]], -1)\n",
    "        lower = tf.concat([z_vals[..., :1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = tf.random.uniform(z_vals.shape)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # Points in space to evaluate model at.\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "          z_vals[..., :, None]  # [N_rays, N_samples, 3]\n",
    "\n",
    "    # Evaluate model at each point.\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)  # [N_rays, N_samples, 4]\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(\n",
    "        raw, z_vals, rays_d)\n",
    "\n",
    "    if N_importance > 0:\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
    "\n",
    "        # Obtain additional integration times to evaluate based on the weights\n",
    "        # assigned to colors in the coarse model.\n",
    "        z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        z_samples = sample_pdf(\n",
    "            z_vals_mid, weights[..., 1:-1], N_importance, det=(perturb == 0.))\n",
    "        z_samples = tf.stop_gradient(z_samples)\n",
    "\n",
    "        # Obtain all points to evaluate color, density at.\n",
    "        z_vals = tf.sort(tf.concat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "              z_vals[..., :, None]  # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        # Make predictions with network_fine.\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(\n",
    "            raw, z_vals, rays_d)\n",
    "\n",
    "    ret = {'rgb_map': rgb_map, 'disp_map': disp_map, 'acc_map': acc_map}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = tf.math.reduce_std(z_samples, -1)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        tf.debugging.check_numerics(ret[k], 'output {}'.format(k))\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def batchify_rays(rays_flat, chunk=1024 * 32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i + chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k: tf.concat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret\n",
    "\n",
    "\n",
    "def render(H, W, focal,\n",
    "           chunk=1024 * 32, rays=None, c2w=None, ndc=True,\n",
    "           near=0., far=1.,\n",
    "           use_viewdirs=False, c2w_staticcam=None,\n",
    "           **kwargs):\n",
    "    \"\"\"Render rays\n",
    "\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for\n",
    "       camera while using other c2w argument for viewing directions.\n",
    "\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, focal, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, focal, c2w_staticcam)\n",
    "\n",
    "        # Make all directions unit magnitude.\n",
    "        # shape: [batch_size, 3]\n",
    "        viewdirs = viewdirs / tf.linalg.norm(viewdirs, axis=-1, keepdims=True)\n",
    "        viewdirs = tf.cast(tf.reshape(viewdirs, [-1, 3]), dtype=tf.float32)\n",
    "\n",
    "    sh = rays_d.shape  # [..., 3]\n",
    "    if ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(\n",
    "            H, W, focal, tf.cast(1., tf.float32), rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = tf.cast(tf.reshape(rays_o, [-1, 3]), dtype=tf.float32)\n",
    "    rays_d = tf.cast(tf.reshape(rays_d, [-1, 3]), dtype=tf.float32)\n",
    "    near, far = near * \\\n",
    "                tf.ones_like(rays_d[..., :1]), far * tf.ones_like(rays_d[..., :1])\n",
    "\n",
    "    # (ray origin, ray direction, min dist, max dist) for each ray\n",
    "    rays = tf.concat([rays_o, rays_d, near, far], axis=-1)\n",
    "    if use_viewdirs:\n",
    "        # (ray origin, ray direction, min dist, max dist, normalized viewing direction)\n",
    "        rays = tf.concat([rays, viewdirs], axis=-1)\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = tf.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k: all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n",
    "\n",
    "\n",
    "def render_path(render_poses, hwf, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor != 0:\n",
    "        # Render downsampled for speed\n",
    "        H = H // render_factor\n",
    "        W = W // render_factor\n",
    "        focal = focal / render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(render_poses):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _ = render(\n",
    "            H, W, focal, chunk=chunk, c2w=c2w[:3, :4], **render_kwargs)\n",
    "        rgbs.append(rgb.numpy())\n",
    "        disps.append(disp.numpy())\n",
    "        if i == 0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        if gt_imgs is not None and render_factor == 0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n",
    "            print(p)\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n",
    "\n",
    "\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\"\"\"\n",
    "\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(\n",
    "            args.multires_views, args.i_embed)\n",
    "    output_ch = 4\n",
    "    skips = [4]\n",
    "    model = init_nerf_model(\n",
    "        D=args.netdepth, W=args.netwidth,\n",
    "        input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "        input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs)\n",
    "    grad_vars = model.trainable_variables\n",
    "    models = {'model': model}\n",
    "\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = init_nerf_model(\n",
    "            D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "            input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "            input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs)\n",
    "        grad_vars += model_fine.trainable_variables\n",
    "        models['model_fine'] = model_fine\n",
    "\n",
    "    def network_query_fn(inputs, viewdirs, network_fn):\n",
    "        return run_network(\n",
    "            inputs, viewdirs, network_fn,\n",
    "            embed_fn=embed_fn,\n",
    "            embeddirs_fn=embeddirs_fn,\n",
    "            netchunk=args.netchunk)\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn': network_query_fn,\n",
    "        'perturb': args.perturb,\n",
    "        'N_importance': args.N_importance,\n",
    "        'network_fine': model_fine,\n",
    "        'N_samples': args.N_samples,\n",
    "        'network_fn': model,\n",
    "        'use_viewdirs': args.use_viewdirs,\n",
    "        'white_bkgd': args.white_bkgd,\n",
    "        'raw_noise_std': args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {\n",
    "        k: render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    if args.ft_path is not None and args.ft_path != 'None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if\n",
    "                 ('model_' in f and 'fine' not in f and 'optimizer' not in f)]\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ft_weights = ckpts[-1]\n",
    "        print('Reloading from', ft_weights)\n",
    "        model.set_weights(np.load(ft_weights, allow_pickle=True))\n",
    "        start = int(ft_weights[-10:-4]) + 1\n",
    "        print('Resetting step to', start)\n",
    "\n",
    "        if model_fine is not None:\n",
    "            ft_weights_fine = '{}_fine_{}'.format(\n",
    "                ft_weights[:-11], ft_weights[-10:])\n",
    "            print('Reloading fine from', ft_weights_fine)\n",
    "            model_fine.set_weights(np.load(ft_weights_fine, allow_pickle=True))\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, models\n",
    "\n",
    "\n",
    "def config_parser():\n",
    "    import configargparse\n",
    "    parser = configargparse.ArgumentParser()\n",
    "    parser.add_argument('--config', is_config_file=True,\n",
    "                        help='config file path')\n",
    "    parser.add_argument(\"--expname\", type=str, help='experiment name')\n",
    "    parser.add_argument(\"--basedir\", type=str, default='./logs/',\n",
    "                        help='where to store ckpts and logs')\n",
    "    parser.add_argument(\"--datadir\", type=str,\n",
    "                        default='./data/llff/fern', help='input data directory')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--netdepth\", type=int, default=8,\n",
    "                        help='layers in network')\n",
    "    parser.add_argument(\"--netwidth\", type=int, default=256,\n",
    "                        help='channels per layer')\n",
    "    parser.add_argument(\"--netdepth_fine\", type=int,\n",
    "                        default=8, help='layers in fine network')\n",
    "    parser.add_argument(\"--netwidth_fine\", type=int, default=256,\n",
    "                        help='channels per layer in fine network')\n",
    "    parser.add_argument(\"--N_rand\", type=int, default=32 * 32 * 4,\n",
    "                        help='batch size (number of random rays per gradient step)')\n",
    "    parser.add_argument(\"--lrate\", type=float,\n",
    "                        default=5e-4, help='learning rate')\n",
    "    parser.add_argument(\"--lrate_decay\", type=int, default=250,\n",
    "                        help='exponential learning rate decay (in 1000s)')\n",
    "    parser.add_argument(\"--chunk\", type=int, default=1024 * 32,\n",
    "                        help='number of rays processed in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--netchunk\", type=int, default=1024 * 64,\n",
    "                        help='number of pts sent through network in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--no_batching\", action='store_true',\n",
    "                        help='only take random rays from 1 image at a time')\n",
    "    parser.add_argument(\"--no_reload\", action='store_true',\n",
    "                        help='do not reload weights from saved ckpt')\n",
    "    parser.add_argument(\"--ft_path\", type=str, default=None,\n",
    "                        help='specific weights npy file to reload for coarse network')\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=None,\n",
    "                        help='fix random seed for repeatability')\n",
    "\n",
    "    # pre-crop options\n",
    "    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n",
    "                        help='number of steps to train on central crops')\n",
    "    parser.add_argument(\"--precrop_frac\", type=float,\n",
    "                        default=.5, help='fraction of img taken for central crops')\n",
    "\n",
    "    # rendering options\n",
    "    parser.add_argument(\"--N_samples\", type=int, default=64,\n",
    "                        help='number of coarse samples per ray')\n",
    "    parser.add_argument(\"--N_importance\", type=int, default=0,\n",
    "                        help='number of additional fine samples per ray')\n",
    "    parser.add_argument(\"--perturb\", type=float, default=1.,\n",
    "                        help='set to 0. for no jitter, 1. for jitter')\n",
    "    parser.add_argument(\"--use_viewdirs\", action='store_true',\n",
    "                        help='use full 5D input instead of 3D')\n",
    "    parser.add_argument(\"--i_embed\", type=int, default=0,\n",
    "                        help='set 0 for default positional encoding, -1 for none')\n",
    "    parser.add_argument(\"--multires\", type=int, default=10,\n",
    "                        help='log2 of max freq for positional encoding (3D location)')\n",
    "    parser.add_argument(\"--multires_views\", type=int, default=4,\n",
    "                        help='log2 of max freq for positional encoding (2D direction)')\n",
    "    parser.add_argument(\"--raw_noise_std\", type=float, default=0.,\n",
    "                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n",
    "\n",
    "    parser.add_argument(\"--render_only\", action='store_true',\n",
    "                        help='do not optimize, reload weights and render out render_poses path')\n",
    "    parser.add_argument(\"--render_test\", action='store_true',\n",
    "                        help='render the test set instead of render_poses path')\n",
    "    parser.add_argument(\"--render_factor\", type=int, default=0,\n",
    "                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n",
    "\n",
    "    # dataset options\n",
    "    parser.add_argument(\"--dataset_type\", type=str, default='llff',\n",
    "                        help='options: llff / blender / deepvoxels')\n",
    "    parser.add_argument(\"--testskip\", type=int, default=8,\n",
    "                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n",
    "\n",
    "    # deepvoxels flags\n",
    "    parser.add_argument(\"--shape\", type=str, default='greek',\n",
    "                        help='options : armchair / cube / greek / vase')\n",
    "\n",
    "    # blender flags\n",
    "    parser.add_argument(\"--white_bkgd\", action='store_true',\n",
    "                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n",
    "    parser.add_argument(\"--half_res\", action='store_true',\n",
    "                        help='load blender synthetic data at 400x400 instead of 800x800')\n",
    "\n",
    "    # llff flags\n",
    "    parser.add_argument(\"--factor\", type=int, default=8,\n",
    "                        help='downsample factor for LLFF images')\n",
    "    parser.add_argument(\"--no_ndc\", action='store_true',\n",
    "                        help='do not use normalized device coordinates (set for non-forward facing scenes)')\n",
    "    parser.add_argument(\"--lindisp\", action='store_true',\n",
    "                        help='sampling linearly in disparity rather than depth')\n",
    "    parser.add_argument(\"--spherify\", action='store_true',\n",
    "                        help='set for spherical 360 scenes')\n",
    "    parser.add_argument(\"--llffhold\", type=int, default=8,\n",
    "                        help='will take every 1/N images as LLFF test set, paper uses 8')\n",
    "\n",
    "    # logging/saving options\n",
    "    parser.add_argument(\"--i_print\", type=int, default=100,\n",
    "                        help='frequency of console printout and metric loggin')\n",
    "    parser.add_argument(\"--i_img\", type=int, default=500,\n",
    "                        help='frequency of tensorboard image logging')\n",
    "    parser.add_argument(\"--i_weights\", type=int, default=10000,\n",
    "                        help='frequency of weight ckpt saving')\n",
    "    parser.add_argument(\"--i_testset\", type=int, default=50000,\n",
    "                        help='frequency of testset saving')\n",
    "    parser.add_argument(\"--i_video\", type=int, default=50000,\n",
    "                        help='frequency of render_poses video saving')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = config_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.random_seed is not None:\n",
    "        print('Fixing random seed', args.random_seed)\n",
    "        np.random.seed(args.random_seed)\n",
    "        tf.compat.v1.set_random_seed(args.random_seed)\n",
    "\n",
    "    # Load data\n",
    "\n",
    "    if args.dataset_type == 'llff':\n",
    "        images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n",
    "                                                                  recenter=True, bd_factor=.75,\n",
    "                                                                  spherify=args.spherify)\n",
    "        hwf = poses[0, :3, -1]\n",
    "        poses = poses[:, :3, :4]\n",
    "        print('Loaded llff', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        if not isinstance(i_test, list):\n",
    "            i_test = [i_test]\n",
    "\n",
    "        if args.llffhold > 0:\n",
    "            print('Auto LLFF holdout,', args.llffhold)\n",
    "            i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "\n",
    "        i_val = i_test\n",
    "        i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                            (i not in i_test and i not in i_val)])\n",
    "\n",
    "        print('DEFINING BOUNDS')\n",
    "        if args.no_ndc:\n",
    "            near = tf.reduce_min(bds) * .9\n",
    "            far = tf.reduce_max(bds) * 1.\n",
    "        else:\n",
    "            near = 0.\n",
    "            far = 1.\n",
    "        print('NEAR FAR', near, far)\n",
    "\n",
    "    elif args.dataset_type == 'blender':\n",
    "        images, poses, render_poses, hwf, i_split = load_blender_data(\n",
    "            args.datadir, args.half_res, args.testskip)\n",
    "        print('Loaded blender', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[..., :3] * images[..., -1:] + (1. - images[..., -1:])\n",
    "        else:\n",
    "            images = images[..., :3]\n",
    "\n",
    "    elif args.dataset_type == 'deepvoxels':\n",
    "\n",
    "        images, poses, render_poses, hwf, i_split = load_dv_data(scene=args.shape,\n",
    "                                                                 basedir=args.datadir,\n",
    "                                                                 testskip=args.testskip)\n",
    "\n",
    "        print('Loaded deepvoxels', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        hemi_R = np.mean(np.linalg.norm(poses[:, :3, -1], axis=-1))\n",
    "        near = hemi_R - 1.\n",
    "        far = hemi_R + 1.\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    # Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    # Create log dir and copy the config file\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg in sorted(vars(args)):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    if args.config is not None:\n",
    "        f = os.path.join(basedir, expname, 'config.txt')\n",
    "        with open(f, 'w') as file:\n",
    "            file.write(open(args.config, 'r').read())\n",
    "\n",
    "    testimgdir = os.path.join(basedir, expname, 'tboard_val_imgs')\n",
    "    os.makedirs(testimgdir, exist_ok=True)\n",
    "\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, models = create_nerf(\n",
    "        args)\n",
    "\n",
    "    bds_dict = {\n",
    "        'near': tf.cast(near, tf.float32),\n",
    "        'far': tf.cast(far, tf.float32),\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        if args.render_test:\n",
    "            # render_test switches to test poses\n",
    "            images = images[i_test]\n",
    "        else:\n",
    "            # Default is smoother render_poses path\n",
    "            images = None\n",
    "\n",
    "        testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format(\n",
    "            'test' if args.render_test else 'path', start))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        print('test poses shape', render_poses.shape)\n",
    "\n",
    "        rgbs, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test,\n",
    "                              gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "        print('Done rendering', testsavedir)\n",
    "        imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'),\n",
    "                         to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "        return\n",
    "\n",
    "    # Create optimizer\n",
    "    lrate = args.lrate\n",
    "    if args.lrate_decay > 0:\n",
    "        lrate = tf.keras.optimizers.schedules.ExponentialDecay(lrate,\n",
    "                                                               decay_steps=args.lrate_decay * 1000, decay_rate=0.1)\n",
    "    optimizer = tf.keras.optimizers.Adam(lrate)\n",
    "    models['optimizer'] = optimizer\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    global_step.assign(start)\n",
    "\n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        # For random ray batching.\n",
    "        #\n",
    "        # Constructs an array 'rays_rgb' of shape [N*H*W, 3, 3] where axis=1 is\n",
    "        # interpreted as,\n",
    "        #   axis=0: ray origin in world space\n",
    "        #   axis=1: ray direction in world space\n",
    "        #   axis=2: observed RGB color of pixel\n",
    "        print('get rays')\n",
    "        # get_rays_np() returns rays_origin=[H, W, 3], rays_direction=[H, W, 3]\n",
    "        # for each pixel in the image. This stack() adds a new dimension.\n",
    "        rays = [get_rays_np(H, W, focal, p) for p in poses[:, :3, :4]]\n",
    "        rays = np.stack(rays, axis=0)  # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.concatenate([rays, images[:, None, ...]], 1)\n",
    "        # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0, 2, 3, 1, 4])\n",
    "        rays_rgb = np.stack([rays_rgb[i]\n",
    "                             for i in i_train], axis=0)  # train images only\n",
    "        # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1, 3, 3])\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    N_iters = 150000#000\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    # Summary writers\n",
    "    writer = tf.summary.create_file_writer(\n",
    "        os.path.join(basedir, 'summaries', expname))\n",
    "    # writer.set_as_default()\n",
    "\n",
    "    for i in range(start, N_iters):\n",
    "        time0 = time.time()\n",
    "\n",
    "        # Sample random ray batch\n",
    "\n",
    "        if use_batching:\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch + N_rand]  # [B, 2+1, 3*?]\n",
    "            batch = tf.transpose(batch, [1, 0, 2])\n",
    "\n",
    "            # batch_rays[i, n, xyz] = ray origin or direction, example_id, 3D position\n",
    "            # target_s[n, rgb] = example_id, observed color.\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                np.random.shuffle(rays_rgb)\n",
    "                i_batch = 0\n",
    "\n",
    "        else:\n",
    "            # Random from one image\n",
    "            img_i = np.random.choice(i_train)\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3, :4]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "                if i < args.precrop_iters:\n",
    "                    dH = int(H // 2 * args.precrop_frac)\n",
    "                    dW = int(W // 2 * args.precrop_frac)\n",
    "                    coords = tf.stack(tf.meshgrid(\n",
    "                        tf.range(H // 2 - dH, H // 2 + dH),\n",
    "                        tf.range(W // 2 - dW, W // 2 + dW),\n",
    "                        indexing='ij'), -1)\n",
    "                    if i < 10:\n",
    "                        print('precrop', dH, dW, coords[0, 0], coords[-1, -1])\n",
    "                else:\n",
    "                    coords = tf.stack(tf.meshgrid(\n",
    "                        tf.range(H), tf.range(W), indexing='ij'), -1)\n",
    "                coords = tf.reshape(coords, [-1, 2])\n",
    "                select_inds = np.random.choice(\n",
    "                    coords.shape[0], size=[N_rand], replace=False)\n",
    "                select_inds = tf.gather_nd(coords, select_inds[:, tf.newaxis])\n",
    "                rays_o = tf.gather_nd(rays_o, select_inds)\n",
    "                rays_d = tf.gather_nd(rays_d, select_inds)\n",
    "                batch_rays = tf.stack([rays_o, rays_d], 0)\n",
    "                target_s = tf.gather_nd(target, select_inds)\n",
    "\n",
    "        #####  Core optimization loop  #####\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Make predictions for color, disparity, accumulated opacity.\n",
    "            rgb, disp, acc, extras = render(\n",
    "                H, W, focal, chunk=args.chunk, rays=batch_rays,\n",
    "                verbose=i < 10, retraw=True, **render_kwargs_train)\n",
    "\n",
    "            # Compute MSE loss between predicted and true RGB.\n",
    "            img_loss = img2mse(rgb, target_s)\n",
    "            trans = extras['raw'][..., -1]\n",
    "            loss = img_loss\n",
    "            psnr = mse2psnr(img_loss)\n",
    "\n",
    "            # Add MSE loss for coarse-grained model\n",
    "            if 'rgb0' in extras:\n",
    "                img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "                loss += img_loss0\n",
    "                psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        gradients = tape.gradient(loss, grad_vars)\n",
    "        optimizer.apply_gradients(zip(gradients, grad_vars))\n",
    "\n",
    "        dt = time.time() - time0\n",
    "\n",
    "        #####           end            #####\n",
    "\n",
    "        # Rest is logging\n",
    "\n",
    "        def save_weights(net, prefix, i):\n",
    "            path = os.path.join(\n",
    "                basedir, expname, '{}_{:06d}.npy'.format(prefix, i))\n",
    "            # np.save(path, net.get_weights())\n",
    "            np.save(path, np.asanyarray(net.get_weights(), object))\n",
    "            print('saved weights at', path)\n",
    "\n",
    "        if i % args.i_weights == 0:\n",
    "            for k in models:\n",
    "                save_weights(models[k], k, i)\n",
    "\n",
    "        if i % args.i_video == 0 and i > 0:\n",
    "\n",
    "            rgbs, disps = render_path(\n",
    "                render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(\n",
    "                basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4',\n",
    "                             to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4',\n",
    "                             to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            if args.use_viewdirs:\n",
    "                render_kwargs_test['c2w_staticcam'] = render_poses[0][:3, :4]\n",
    "                rgbs_still, _ = render_path(\n",
    "                    render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "                render_kwargs_test['c2w_staticcam'] = None\n",
    "                imageio.mimwrite(moviebase + 'rgb_still.mp4',\n",
    "                                 to8b(rgbs_still), fps=30, quality=8)\n",
    "\n",
    "        if i % args.i_testset == 0 and i > 0:\n",
    "            testsavedir = os.path.join(\n",
    "                basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            render_path(poses[i_test], hwf, args.chunk, render_kwargs_test,\n",
    "                        gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "        if i % args.i_print == 0 or i < 10:\n",
    "\n",
    "            print(expname, i, psnr.numpy(), loss.numpy(), global_step.numpy())\n",
    "            print('iter time {:.05f}'.format(dt))\n",
    "            # with tf.summary.record_summaries_every_n_global_steps(args.i_print):\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "                tf.summary.scalar('psnr', psnr, step=i)\n",
    "                tf.summary.histogram('tran', trans, step=i)\n",
    "                if args.N_importance > 0:\n",
    "                    tf.summary.scalar('psnr0', psnr0, step=i)\n",
    "\n",
    "            if i % args.i_img == 0:\n",
    "\n",
    "                # Log a rendered validation view to Tensorboard\n",
    "                img_i = np.random.choice(i_val)\n",
    "                target = images[img_i]\n",
    "                pose = poses[img_i, :3, :4]\n",
    "\n",
    "                rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose,\n",
    "                                                **render_kwargs_test)\n",
    "\n",
    "                psnr = mse2psnr(img2mse(rgb, target))\n",
    "\n",
    "                # Save out the validation image for Tensorboard-free monitoring\n",
    "\n",
    "                # testimgdir = os.path.join(basedir, expname, 'tboard_val_imgs')\n",
    "                # if i==0:\n",
    "                #     os.makedirs(testimgdir, exist_ok=True)\n",
    "                imageio.imwrite(os.path.join(testimgdir, '{:06d}.png'.format(i)), to8b(rgb))\n",
    "\n",
    "                # with tf.summary.record_summaries_every_n_global_steps(args.i_img):\n",
    "                with writer.as_default():\n",
    "                    tf.summary.image('rgb', to8b(rgb)[tf.newaxis], step=i)\n",
    "                    tf.summary.image(\n",
    "                        'disp', disp[tf.newaxis, ..., tf.newaxis], step=i)\n",
    "                    tf.summary.image(\n",
    "                        'acc', acc[tf.newaxis, ..., tf.newaxis], step=i)\n",
    "\n",
    "                    tf.summary.scalar('psnr_holdout', psnr, step=i)\n",
    "                    tf.summary.image('rgb_holdout', target[tf.newaxis], step=i)\n",
    "\n",
    "                if args.N_importance > 0:\n",
    "                    # with tf.summary.record_summaries_every_n_global_steps(args.i_img):\n",
    "                    with writer.as_default():\n",
    "                        tf.summary.image(\n",
    "                            'rgb0', to8b(extras['rgb0'])[tf.newaxis], step=i)\n",
    "                        tf.summary.image(\n",
    "                            'disp0', extras['disp0'][tf.newaxis, ..., tf.newaxis], step=i)\n",
    "                        tf.summary.image(\n",
    "                            'z_std', extras['z_std'][tf.newaxis, ..., tf.newaxis], step=i)\n",
    "\n",
    "        global_step.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fac8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nerf)",
   "language": "python",
   "name": "nerf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
